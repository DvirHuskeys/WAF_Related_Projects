<story-context id="{bmad_folder}/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>2</epicId>
    <storyId>6</storyId>
    <title>Data Export Utilities</title>
    <status>ready-for-dev</status>
    <generatedAt>2025-11-30</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/sprint-artifacts/2-6-data-export-utilities.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>As an analyst</asA>
    <iWant>I want to export enriched domains and rule libraries as CSV/Parquet</iWant>
    <soThat></soThat>
    <tasks>      <![CDATA[
      - [ ] Export helper (AC: 1–2)  
  - [ ] Implement `backend/services/export.py` with functions `export_domains(path, format)` and `export_rules(…)`.  
  - [ ] Use DuckDB `COPY` statements for CSV, `duckdb.sql("COPY ... TO 'file.parquet' (FORMAT PARQUET)")` for Parquet.  
- [ ] CLI wiring (AC: 1–2 & 5)  
  - [ ] Create `scripts/export_data.py` with Typer options (`--domains/--rules`, `--format csv|parquet`).  
  - [ ] Document usage in README/Makefile.  
- [ ] Streamlit buttons (AC: 3 & 4)  
  - [ ] Add “Download CSV” + “Download Parquet” buttons to Radar page; show toast when ready.  
  - [ ] Footnote includes freshness warning + job reference (read from job log).  
- [ ] Tests / verification  
  - [ ] Smoke test to ensure files land under `exports/` and contain expected columns.
      ]]></tasks>
  </story>

  <acceptanceCriteria>      <![CDATA[
      1. Given enrichment data exists, when I click “Download Domains CSV” (or run `python scripts/export_data.py --domains`) then a file is saved under `exports/` with ISO timestamp in the filename and headers matching the DuckDB schema.
2. Same export command supports `--parquet` option for domains and rules; Parquet files include schema metadata.
3. Streamlit UI exposes download buttons for domains and rules that call the same helper, displaying success/error toasts.
4. Exports respect freshness warnings (append note if data older than SLA) and include a footer referencing the job ID from Story 2.4.
5. README documents how to export manually via CLI.
      ]]></acceptanceCriteria>

  <artifacts>
    <docs>      <doc path="docs/sprint-artifacts/2-6-data-export-utilities.md" title="Data Export Utilities" section="Story Draft" snippet="As an analyst, I want to export enriched domains and rule libraries as CSV/Parquet, so I can explore the data in notebooks or share snapshots without giving direct DB access." />
      <doc path="docs/epics.md" title="WAF Security - Epic Breakdown" section="Story 2.6" snippet="As an analyst, I want to export enriched data and rule libraries as CSV/Parquet so I can slice them in notebooks. **Acceptance Criteria** • **Given** I click “Download domains CSV” or call a CLI flag • **When** DuckDB has data • **Then** a file saves under `exports/` with ISO timestamp in filename and column headers per schema **Prerequisites** Stories…" />
      <doc path="docs/prd.md" title="Product Requirements Document" section="Executive Summary" snippet="WAF Security establishes a local-first “insight spine” that combines WAFtotal (vendor-agnostic managed-rule transparency) with GTM Radar (domain fingerprinting + storytelling) so every security, architecture, and GTM conversation starts with evidence instead of guesswork. By harvesting domain signals, normalizing Cloudflare/AWS/Akamai rule packs, and…" />
      <doc path="docs/architecture.md" title="Architecture Specification" section="Project Context &amp; Goals" snippet="- **Deployment Reality:** Local-first sandbox meant to run entirely on a lone laptop (Streamlit UI, FastAPI services, DuckDB file storage). No cloud infra, no Postgres/React rewrite—lightweight scripts + HTML artifacts are sufficient so iteration stays fast. - **Primary Outcomes:** 1. Fingerprint domains (CLI + FastAPI) and persist results in DuckDB. 2.…" />
      <doc path="docs/product-brief-WAF Security-2025-11-30.md" title="Product Brief" section="Executive Summary" snippet="WAF Security is building an internal “insight spine” that fuses two sibling ideas—**WAFtotal**, a vendor-agnostic managed-rule transparency experience, and **GTM Radar**, a domain fingerprinting + storytelling engine for sales and marketing teams. Together they deliver three promises: (1) decode any prospect’s WAF/CDN posture in minutes, (2) expose…" />
      <doc path="docs/research-market-2025-11-30.md" title="Market/Domain Research" section="Executive Summary" snippet="- The global web application firewall (WAF) market is scaling aggressively: SNS Insider pegs it at **USD 6.35 B in 2023 with a path to USD 28.6 B by 2032 (18.2% CAGR)**, while ResearchAndMarkets estimates **USD 6.22 B in 2024 growing to USD 19.2 B by 2033 (13.3% CAGR)**, giving us a triangulated TAM envelope of USD 19–29 B within a decade. - [Source: SNS…" /></docs>
    <code>      <code path="scripts/domain_enrich.py" kind="cli" symbol="main" lines="20-87" reason="Implements CSV ingestion pipeline for enrichment CLIs." />
      <code path="backend/services/fingerprint.py" kind="service" symbol="detect_stack" lines="21-51" reason="Detects WAF/CDN stacks and scoring heuristics reused across Epic 2." />
      <code path="backend/services/storage.py" kind="service" symbol="get_connection" lines="11-68" reason="Persists enrichment + rule sync results mentioned in pipeline stories." /></code>
    <dependencies>      <dependency name="duckdb" version="1.1.3" scope="storage" />
      <dependency name="typer" version="0.12.5" scope="cli" />
      <dependency name="wafw00f" version="2.2.0" scope="security" />
      <dependency name="rich" version="13.9.2" scope="cli" /></dependencies>
  </artifacts>

  <constraints>      <![CDATA[
      - Aligns with PRD FR18; export path should be configurable via env.  
- Consider cleaning up old exports (optional).  
- Use same is_stale helper from Story 2.5.

### Project Structure Notes

- New folder `exports/` should be gitignored but created by script if missing.  
- Keep CLI naming consistent with other scripts.

### References

- [Source: docs/epics.md#story-26-data-export-utilities]  
- [Source: docs/prd.md#functional-requirements FR18]
      ]]></constraints>
  <interfaces>      <interface name="domain_enrich CLI" kind="CLI command" signature="python scripts/domain_enrich.py data/samples/domains.csv [--limit]" path="scripts/domain_enrich.py" /></interfaces>
  <tests>
    <standards>      <![CDATA[
      - [x] domain_enrich.py run Command: `python scripts/domain_enrich.py data/samples/domains.csv` Result: Generated four enriched records in DuckDB, offline heuristics used. - [x] rule_sync/run.py Command: `python scripts/rule_sync/run.py cloudflare --source data/rules/cloudflare_sample.json` Result: Loaded two sample managed rules. - [x] API schema test…
      ]]></standards>
    <locations>      <![CDATA[
      tests/, scripts/, ui/
      ]]></locations>
    <ideas>      <![CDATA[
      AC1: Verify Given enrichment data exists, when I click “Download Domains CSV” (or run `python scripts/export_data.py --domains`) then a file is saved under `exports/` with ISO timestamp in the filename and headers matching the DuckDB schema.; AC2: Verify Same export command supports `--parquet` option for domains and rules; Parquet files include schema metadata.; AC3: Verify Streamlit UI exposes download buttons for domains and rules that call the same helper, displaying success/error toasts.; AC4: Verify Exports respect freshness warnings (append note if data older than SLA) and include a footer referencing the job ID from Story 2.4.; AC5: Verify README documents how to export manually via CLI.
      ]]></ideas>
  </tests>
</story-context>
